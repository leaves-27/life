1.介绍
在这一课，我们将介绍5个非常有效的机器学习算法用于回归任务。它们每一个也都和分类相当。

好了，现你在开始介绍这个5个算法。我们的目标是解释一些基本的概念（例如：正则化、聚类、自动化特征选择），而不是给你一个长长的算法列表。这些基本概念将会使你懂得为什么一些算法运行效果比其他的效果要好。

2.线性回归的缺点

为了介绍一些高级算法的来龙去脉，让我们从讨论基本的线性回归开始。线性回归模型是非常常见的，但是有严重的缺点。

简单的线性回归模型适合于区间连续的情况（技术上依赖于大量的相同特征的值）。在实际应用中，它们很少完美的运行。对于大多数机器学习问题，我们实际上推荐忽略它们。

它们的主要优势是容易解释和理解。然而，我们的目标是建立一个可以准确预测的模型。

在这一点上，简单的线性回归有两个主要的缺点：
  对于许多输入的特征，它易于过度拟合。
  它不能表示非线性关系。

让我们看看我们如何解决第一个缺点。

3.正则化

这是提高模型的性能的第一个手段。在许多机器语言课程中，它被认为是高级的，但是它是很容易理解和实现的。

线性模型的第一个缺点是对于许多输入的特征，它易于过度拟合。让我们看一个极端的例子，解释发生了什么：
  在你的训练数据中，我们假设有100个观察对象。假设我们也有100个特征。如果你对于那100个特征用线性回归模型，你可以完全记住这个训练数据。每个系数将简单的记住这个观察对象。这个模型在训练数据中将是完全正确的，但是在隐藏的数据中，它就很难得到想要的结果。因为它不是学习了真正的模式，而是仅仅记住了训练数据中的噪音。正则化是一个通过人工调整模型系数来阻止过度拟合的技术。
  
  可以降低这个很大的系数（通过调节它们）。
  也可以完全移除这个特征（通过设置他们的特征为0）。
  这个调整的力度是可以调节的。

4.正则化后的回归
有三种常见正则化线性回归算法。它们代表最小的绝对收缩和选择操作符。

Lasso
Lasso回归调整了系数的绝对大小。事实上，导致系数可以被精确控制到0.
再次，Lasso回归提出了自动化特征选择因为它可以完全的移除一些特征。
记住，这个调整的力度应该是可调整的。
大力度的调整会导致很多系数趋向于0。

Ridge
Ridge代表用不正确的方法吃葡萄柚（像小孩...他就是ridage）。

Ridge 回归调整系数的平方大小。事实上，导致系数变的更小，但是它不会让它们趋向于0。
换句话说，Ridge提供了特征收缩。再次强调下，控制的力度应该是可调整的。
强力的调整会导致系数趋近于0。

Elastic-Net
Elastic-Net是介于Lasso 和 Ridge之间的折中方案。
Elastic-Net是绝对大小和平方大小都进行调整。这两个调整类型的比例应该是可调整的。
整个力度也应该是可调整的。
好了，没有更好的控制方法了。它事实上依赖于数据集和问题。我们推荐尝试不同的用一个调整范围作为调整过程的一部分的算法。在明天的课程中，我们再详细讲解。

5.决策树
好了，我们刚刚看了三个防止线性回归过度拟合的算法。但是，你是否记得，线性回归有两个主要的缺点会使它的可用性变差：
  用许多输入特征使它易于过度拟合。
  它不能很容易的表示非线性关系。
  
我们如何解决第二个缺点？好了，我们暂时不要看线性模型，看看一类新的算法。
决策树模型数据作为各层分支中的一个。它们使整个树的分支的每一个叶子做出预测。
由于它们的分支结构，决策树可以很容易的表示非线性关系。
例如，让我们对单户住宅进行假设，房间数越多价格越高。然而对于度假套房，房间数越少价格越高。（它是城市和农村的一种表现）
对于线性模型捕获这个相互关系是困难的除非你明确的添加了一个相互关联的术语(即你可以提前预测它)。
另一方面，决策树可以自然地捕获这个相互关系。
不幸的是，决策树也有一个主要缺点使它的可用性变差。如果你允许它们无限的增长，它们可以完全记住所有训练数据，仅仅只要创建更多的分支就可以了。

因此，单纯的无限决策树是非常容易过度拟合的。
所以，防止过度拟合训练数据的同时，我们如何发挥决策树的灵活性呢？

6.集成是一种结合多个单一模型预测结果的机器学习方法。
集成的方法各种各样，但是有两个最常见的是：
Bagging试图减少过渡拟合的复杂模型变化。它并行训练大量的健壮学习者。一个健壮的学习者是一个相对不受限制的模型。
Boosting试图增强简单模型的预测灵活性。它依次训练大量的弱学习者。
一个弱学习者是一个受限的模型（例如：你可以限制每一个决策树的最大深度）。
每一个依次训练的弱学习者集中于从它前一个犯的错误中学习。
然后Boosting把所有弱学习者合并成一个健壮的学习者。

bagging和boosting都是集成方法，他们从不同角度来解决问题。Bagging用多个基础模型，然后试图整合它们的预测结果来解决问题。而boosting用一个简单的基础模型，然后试图一步步增强他们复杂性的来解决问题。

7.树集成
集成是一个总称的方法，当基础模型是决策树时，它们有具体的方法：随机森林和boosted trees！

随机森林训练大量强壮的决策树并把它和bagging的预测结果结合。

除此之外，随机森林有两个随机源：
每棵树仅允许从分离的特征的一个随机子集中选择(引起特征选择)。
每棵树仅仅基于观察对象的一个随机子集被训练（一个被叫做重取样的过程）。

在实际练习中，随机森林效果非常好。

它们常常锤炼许多其他的需要花费好几周来开发的模型。
它们是完美的瑞士军刀式算法，大多数情况下总是得到正确的结果。
它们没有许多需要调整的复杂参数。

Boosted trees
Boosted trees训练一系列弱的、有约束性的决策树，结合boosting的预测得出结果。

每棵树被允许一个可以调整的最大深度。
在这个序列中的每棵树尝试纠正它前一棵树的预测错误。
在实际练习中，boosted trees有很大的提升空间。
它们常常用来锤炼许多调整参数后的类型的模型。
和随机森林模型相比，它们有更多复杂的参数需要调整。

8.结论
“哟”，讲的好多了！如果你需要或者感觉信息量太大没有完全理解，可以再读一读这一课。

关键要点导读：
  最有效的算法常常是综合运用正则化、自动化特征选择、表达非线性关系的能力和集成多种手段。
  常见的此类算法有：
    Lasso 回归
    Ridge 回归
    Elastic-Net（弹性网络）
    随机森林
    Boosted tree
  这儿是一个确保你掌握了所有知识点的小测验：
  线性回归的两个最大缺点是什么？
  你是如何定位第一个缺点的（通过哪种途径，哪种算法）？
  你是如何定位第二个缺点的（通过哪种途径，哪种算法）？
  正则化罚项的两种类型是什么？在实际练习中你是如何做的？
  集成的两个方法是什么？它们如何工作？

作为最后的注意点，我们不得不为这一课设定两个目标：
1.介绍现代机器学习的核心机制
2.按一定路径看看、应用下那几个使用那些机制的算法。在我们的高级课程中，我们会更详细的用大量的练习来讲解它们。

再说一次，如果有问题或建议，随时邮件联系我们。我们一定回复每一封邮件。

既然在你的工具箱中，你已经有了大量的算法。明天的部分我们将教你如何正确的应用它们训练模型。

